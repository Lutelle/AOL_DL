{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f558673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.20.0\n",
      "GPU is available!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765708987.176882  227598 gpu_device.cc:2020] Created device /device:GPU:0 with 13551 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Reshape, Conv2D, Conv2DTranspose, Flatten, LeakyReLU, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "if tf.test.is_gpu_available():\n",
    "    print(\"GPU is available!\")\n",
    "else:\n",
    "    print(\"GPU not available, training on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6006dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '/mnt/d/Code/DeepLearning_AOL1/data/anime_faces' #Ganti sesuai kebutuhan\n",
    "IMAGE_SIZE = 64 \n",
    "BATCH_SIZE = 128\n",
    "NOISE_DIM = 100 \n",
    "\n",
    "CHECKPOINT_DIR = 'mnt/d/outputs/models' #Ganti sesuai kebutuhan\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True) \n",
    "\n",
    "EPOCH_CHECKPOINT_SAVE_INTERVAL = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4196100e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 63565 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765709047.898939  227598 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13551 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of one batch of images: (128, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    \n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        DATASET_PATH,\n",
    "        labels=None, \n",
    "        image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        interpolation='bilinear'\n",
    "    )\n",
    "    \n",
    "    def normalize_img(image):\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image = (image - 127.5) / 127.5\n",
    "        return image\n",
    "\n",
    "    dataset = dataset.map(normalize_img)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "try:\n",
    "    train_dataset = load_data()\n",
    "except Exception as e:\n",
    "    print(f\"Error memuat data. Pastikan DATASET_PATH benar dan berisi gambar: {e}\")\n",
    "\n",
    "try:\n",
    "    for x in train_dataset.take(1):\n",
    "        print(\"Shape of one batch of images:\", x.shape)\n",
    "        break\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df2098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(noise_dim=NOISE_DIM):\n",
    "    model = Sequential(name='Generator')\n",
    "    \n",
    "    model.add(Dense(4 * 4 * 512, use_bias=False, input_shape=(noise_dim,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Reshape((4, 4, 512))) \n",
    "    \n",
    "    model.add(Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    \n",
    "    model.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    \n",
    "    model.add(Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "generator = build_generator(NOISE_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc0f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(image_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)):\n",
    "    model = Sequential(name='Discriminator')\n",
    "\n",
    "    model.add(Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=image_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1)) \n",
    "\n",
    "    return model\n",
    "\n",
    "discriminator = build_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a536a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "REAL_TARGET = 1.0\n",
    "FAKE_TARGET = 0.0\n",
    "GEN_TARGET = 1.0\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_labels = tf.ones_like(real_output) * REAL_TARGET\n",
    "    real_loss = mse_loss(real_labels, real_output)\n",
    "    \n",
    "    fake_labels = tf.ones_like(fake_output) * FAKE_TARGET\n",
    "    fake_loss = mse_loss(fake_labels, fake_output)\n",
    "    \n",
    "    return real_loss + fake_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    gen_labels = tf.ones_like(fake_output) * GEN_TARGET\n",
    "    return mse_loss(gen_labels, fake_output)\n",
    "\n",
    "generator_optimizer = Adam(1e-4) \n",
    "discriminator_optimizer = Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6df2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = tf.random.normal([16, NOISE_DIM])\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "\n",
    "manager = tf.train.CheckpointManager(checkpoint, CHECKPOINT_DIR, max_to_keep=5)\n",
    "\n",
    "def load_latest_checkpoint():\n",
    "    global initial_epoch\n",
    "    initial_epoch = 0\n",
    "    \n",
    "    latest_checkpoint = manager.latest_checkpoint\n",
    "    \n",
    "    if latest_checkpoint:\n",
    "        print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Checkpoint ditemukan: {latest_checkpoint}\")\n",
    "        \n",
    "        status = checkpoint.restore(latest_checkpoint)\n",
    "        status.expect_partial() \n",
    "        \n",
    "        print(\"Model, optimizers, dan state telah dipulihkan (secara parsial).\")\n",
    "        \n",
    "        try:\n",
    "            checkpoint_index = int(latest_checkpoint.split('-')[-1]) \n",
    "            initial_epoch = checkpoint_index * EPOCH_CHECKPOINT_SAVE_INTERVAL\n",
    "            print(f\"Melanjutkan training dari Epoch: {initial_epoch + 1}\")\n",
    "        except:\n",
    "            initial_epoch = 0\n",
    "            print(\"Tidak dapat menentukan nomor epoch sebelumnya. Melanjutkan training dari awal.\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Tidak ada checkpoint ditemukan. Memulai training dari awal (Epoch 1).\")\n",
    "    \n",
    "    return initial_epoch\n",
    "\n",
    "initial_epoch = 0 \n",
    "\n",
    "\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    predictions = model(test_input, training=False)\n",
    "    predictions = (predictions * 0.5) + 0.5\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(tf.clip_by_value(predictions[i, :, :, :].numpy(), 0, 1))\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.savefig(f'./{CHECKPOINT_DIR}/image_at_epoch_{epoch:04d}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d9e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    \n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c992af85",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    \n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfedb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs_to_run):\n",
    "    global initial_epoch \n",
    "    initial_epoch = load_latest_checkpoint()\n",
    "    \n",
    "    target_total_epochs = initial_epoch + epochs_to_run\n",
    "    \n",
    "    print(f\"Memulai pelatihan baru. Akan berjalan selama {epochs_to_run} epoch (hingga Epoch {target_total_epochs}).\")\n",
    "\n",
    "    G_losses = []\n",
    "    D_losses = []\n",
    "\n",
    "    for epoch in range(initial_epoch, target_total_epochs): \n",
    "        start = time.time()\n",
    "        current_epoch_number = epoch + 1 \n",
    "        \n",
    "        gen_loss_list = []\n",
    "        disc_loss_list = []\n",
    "\n",
    "        for image_batch in dataset:\n",
    "            g_loss, d_loss = train_step(image_batch)\n",
    "            gen_loss_list.append(g_loss.numpy())\n",
    "            disc_loss_list.append(d_loss.numpy())\n",
    "            \n",
    "        avg_gen_loss = np.mean(gen_loss_list)\n",
    "        avg_disc_loss = np.mean(disc_loss_list)\n",
    "        \n",
    "        G_losses.append(avg_gen_loss)\n",
    "        D_losses.append(avg_disc_loss)\n",
    "\n",
    "        generate_and_save_images(generator, current_epoch_number, seed)\n",
    "        \n",
    "        if current_epoch_number % EPOCH_CHECKPOINT_SAVE_INTERVAL == 0:\n",
    "            save_path = manager.save()\n",
    "            print(f\"Checkpoint disimpan di: {save_path}\")\n",
    "\n",
    "        print (f'Epoch {current_epoch_number}, Gen Loss: {avg_gen_loss:.4f}, Disc Loss: {avg_disc_loss:.4f}, Time: {time.time()-start:.2f} sec')\n",
    "\n",
    "    generate_and_save_images(generator, target_total_epochs, seed)\n",
    "    \n",
    "    return G_losses, D_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc74f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_TO_RUN = 100 \n",
    "history_G_loss, history_D_loss = train(train_dataset, EPOCHS_TO_RUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d99feba",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = initial_epoch + 1 \n",
    "epochs_ran = len(history_G_loss)\n",
    "epochs_list = list(range(start_epoch, start_epoch + epochs_ran))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs_list, history_G_loss, label='Generator Loss', marker='o', linestyle='-')\n",
    "plt.plot(epochs_list, history_D_loss, label='Discriminator Loss', marker='x', linestyle='--')\n",
    "\n",
    "plt.title('Generator and Discriminator Loss over Epochs (LSGAN)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Value (Mean Squared Error)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
